//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-37061995
// Cuda compilation tools, release 13.1, V13.1.115
// Based on NVVM 7.0.1
//

.version 9.1
.target sm_80
.address_size 64

	// .globl	_Z20ann_wmma_layer1_reluPK13__nv_bfloat16S1_PKfPf

.visible .entry _Z20ann_wmma_layer1_reluPK13__nv_bfloat16S1_PKfPf(
	.param .u64 _Z20ann_wmma_layer1_reluPK13__nv_bfloat16S1_PKfPf_param_0,
	.param .u64 _Z20ann_wmma_layer1_reluPK13__nv_bfloat16S1_PKfPf_param_1,
	.param .u64 _Z20ann_wmma_layer1_reluPK13__nv_bfloat16S1_PKfPf_param_2,
	.param .u64 _Z20ann_wmma_layer1_reluPK13__nv_bfloat16S1_PKfPf_param_3
)
{
	.reg .f32 	%f<34>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [_Z20ann_wmma_layer1_reluPK13__nv_bfloat16S1_PKfPf_param_0];
	ld.param.u64 	%rd2, [_Z20ann_wmma_layer1_reluPK13__nv_bfloat16S1_PKfPf_param_1];
	ld.param.u64 	%rd3, [_Z20ann_wmma_layer1_reluPK13__nv_bfloat16S1_PKfPf_param_2];
	ld.param.u64 	%rd4, [_Z20ann_wmma_layer1_reluPK13__nv_bfloat16S1_PKfPf_param_3];
	cvta.to.global.u64 	%rd5, %rd3;
	mov.u32 	%r1, 16;
	wmma.load.a.sync.aligned.row.m16n16k16.bf16 	{%r2, %r3, %r4, %r5}, [%rd2], %r1;
	wmma.load.b.sync.aligned.col.m16n16k16.bf16 	{%r6, %r7, %r8, %r9}, [%rd1], %r1;
	mov.f32 	%f1, 0f00000000;
	wmma.mma.sync.aligned.row.col.m16n16k16.f32.bf16.bf16.f32 {%f2, %f3, %f4, %f5, %f6, %f7, %f8, %f9}, {%r2, %r3, %r4, %r5}, {%r6, %r7, %r8, %r9}, {%f1, %f1, %f1, %f1, %f1, %f1, %f1, %f1};
	cvta.to.global.u64 	%rd6, %rd4;
	ld.global.f32 	%f10, [%rd5];
	add.f32 	%f11, %f10, %f2;
	max.f32 	%f12, %f11, %f1;
	ld.global.f32 	%f13, [%rd5+4];
	add.f32 	%f14, %f13, %f3;
	max.f32 	%f15, %f14, %f1;
	ld.global.f32 	%f16, [%rd5+8];
	add.f32 	%f17, %f16, %f4;
	max.f32 	%f18, %f17, %f1;
	ld.global.f32 	%f19, [%rd5+12];
	add.f32 	%f20, %f19, %f5;
	max.f32 	%f21, %f20, %f1;
	ld.global.f32 	%f22, [%rd5+16];
	add.f32 	%f23, %f22, %f6;
	max.f32 	%f24, %f23, %f1;
	ld.global.f32 	%f25, [%rd5+20];
	add.f32 	%f26, %f25, %f7;
	max.f32 	%f27, %f26, %f1;
	ld.global.f32 	%f28, [%rd5+24];
	add.f32 	%f29, %f28, %f8;
	max.f32 	%f30, %f29, %f1;
	ld.global.f32 	%f31, [%rd5+28];
	add.f32 	%f32, %f31, %f9;
	max.f32 	%f33, %f32, %f1;
	wmma.store.d.sync.aligned.row.m16n16k16.global.f32 	[%rd6], {%f12, %f15, %f18, %f21, %f24, %f27, %f30, %f33}, %r1;
	ret;

}
	// .globl	_Z22ann_wmma_layer2_linearPK13__nv_bfloat16S1_PKfPf
.visible .entry _Z22ann_wmma_layer2_linearPK13__nv_bfloat16S1_PKfPf(
	.param .u64 _Z22ann_wmma_layer2_linearPK13__nv_bfloat16S1_PKfPf_param_0,
	.param .u64 _Z22ann_wmma_layer2_linearPK13__nv_bfloat16S1_PKfPf_param_1,
	.param .u64 _Z22ann_wmma_layer2_linearPK13__nv_bfloat16S1_PKfPf_param_2,
	.param .u64 _Z22ann_wmma_layer2_linearPK13__nv_bfloat16S1_PKfPf_param_3
)
{
	.reg .f32 	%f<26>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [_Z22ann_wmma_layer2_linearPK13__nv_bfloat16S1_PKfPf_param_0];
	ld.param.u64 	%rd2, [_Z22ann_wmma_layer2_linearPK13__nv_bfloat16S1_PKfPf_param_1];
	ld.param.u64 	%rd3, [_Z22ann_wmma_layer2_linearPK13__nv_bfloat16S1_PKfPf_param_2];
	ld.param.u64 	%rd4, [_Z22ann_wmma_layer2_linearPK13__nv_bfloat16S1_PKfPf_param_3];
	cvta.to.global.u64 	%rd5, %rd3;
	mov.u32 	%r1, 16;
	wmma.load.a.sync.aligned.row.m16n16k16.bf16 	{%r2, %r3, %r4, %r5}, [%rd2], %r1;
	wmma.load.b.sync.aligned.col.m16n16k16.bf16 	{%r6, %r7, %r8, %r9}, [%rd1], %r1;
	mov.f32 	%f1, 0f00000000;
	wmma.mma.sync.aligned.row.col.m16n16k16.f32.bf16.bf16.f32 {%f2, %f3, %f4, %f5, %f6, %f7, %f8, %f9}, {%r2, %r3, %r4, %r5}, {%r6, %r7, %r8, %r9}, {%f1, %f1, %f1, %f1, %f1, %f1, %f1, %f1};
	cvta.to.global.u64 	%rd6, %rd4;
	ld.global.f32 	%f10, [%rd5];
	add.f32 	%f11, %f10, %f2;
	ld.global.f32 	%f12, [%rd5+4];
	add.f32 	%f13, %f12, %f3;
	ld.global.f32 	%f14, [%rd5+8];
	add.f32 	%f15, %f14, %f4;
	ld.global.f32 	%f16, [%rd5+12];
	add.f32 	%f17, %f16, %f5;
	ld.global.f32 	%f18, [%rd5+16];
	add.f32 	%f19, %f18, %f6;
	ld.global.f32 	%f20, [%rd5+20];
	add.f32 	%f21, %f20, %f7;
	ld.global.f32 	%f22, [%rd5+24];
	add.f32 	%f23, %f22, %f8;
	ld.global.f32 	%f24, [%rd5+28];
	add.f32 	%f25, %f24, %f9;
	wmma.store.d.sync.aligned.row.m16n16k16.global.f32 	[%rd6], {%f11, %f13, %f15, %f17, %f19, %f21, %f23, %f25}, %r1;
	ret;

}
	// .globl	_Z15ann_wmma_argmaxPKfPi
.visible .entry _Z15ann_wmma_argmaxPKfPi(
	.param .u64 _Z15ann_wmma_argmaxPKfPi_param_0,
	.param .u64 _Z15ann_wmma_argmaxPKfPi_param_1
)
{
	.reg .pred 	%p<17>;
	.reg .f32 	%f<31>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [_Z15ann_wmma_argmaxPKfPi_param_0];
	ld.param.u64 	%rd2, [_Z15ann_wmma_argmaxPKfPi_param_1];
	mov.u32 	%r1, %tid.x;
	setp.ne.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB2_2;

	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	ld.global.f32 	%f1, [%rd3+64];
	ld.global.f32 	%f2, [%rd3];
	setp.gt.f32 	%p2, %f1, %f2;
	selp.f32 	%f3, %f1, %f2, %p2;
	selp.u32 	%r2, 1, 0, %p2;
	ld.global.f32 	%f4, [%rd3+128];
	setp.gt.f32 	%p3, %f4, %f3;
	selp.f32 	%f5, %f4, %f3, %p3;
	selp.b32 	%r3, 2, %r2, %p3;
	ld.global.f32 	%f6, [%rd3+192];
	setp.gt.f32 	%p4, %f6, %f5;
	selp.f32 	%f7, %f6, %f5, %p4;
	selp.b32 	%r4, 3, %r3, %p4;
	ld.global.f32 	%f8, [%rd3+256];
	setp.gt.f32 	%p5, %f8, %f7;
	selp.f32 	%f9, %f8, %f7, %p5;
	selp.b32 	%r5, 4, %r4, %p5;
	ld.global.f32 	%f10, [%rd3+320];
	setp.gt.f32 	%p6, %f10, %f9;
	selp.f32 	%f11, %f10, %f9, %p6;
	selp.b32 	%r6, 5, %r5, %p6;
	ld.global.f32 	%f12, [%rd3+384];
	setp.gt.f32 	%p7, %f12, %f11;
	selp.f32 	%f13, %f12, %f11, %p7;
	selp.b32 	%r7, 6, %r6, %p7;
	ld.global.f32 	%f14, [%rd3+448];
	setp.gt.f32 	%p8, %f14, %f13;
	selp.f32 	%f15, %f14, %f13, %p8;
	selp.b32 	%r8, 7, %r7, %p8;
	ld.global.f32 	%f16, [%rd3+512];
	setp.gt.f32 	%p9, %f16, %f15;
	selp.f32 	%f17, %f16, %f15, %p9;
	selp.b32 	%r9, 8, %r8, %p9;
	ld.global.f32 	%f18, [%rd3+576];
	setp.gt.f32 	%p10, %f18, %f17;
	selp.f32 	%f19, %f18, %f17, %p10;
	selp.b32 	%r10, 9, %r9, %p10;
	ld.global.f32 	%f20, [%rd3+640];
	setp.gt.f32 	%p11, %f20, %f19;
	selp.f32 	%f21, %f20, %f19, %p11;
	selp.b32 	%r11, 10, %r10, %p11;
	ld.global.f32 	%f22, [%rd3+704];
	setp.gt.f32 	%p12, %f22, %f21;
	selp.f32 	%f23, %f22, %f21, %p12;
	selp.b32 	%r12, 11, %r11, %p12;
	ld.global.f32 	%f24, [%rd3+768];
	setp.gt.f32 	%p13, %f24, %f23;
	selp.f32 	%f25, %f24, %f23, %p13;
	selp.b32 	%r13, 12, %r12, %p13;
	ld.global.f32 	%f26, [%rd3+832];
	setp.gt.f32 	%p14, %f26, %f25;
	selp.f32 	%f27, %f26, %f25, %p14;
	selp.b32 	%r14, 13, %r13, %p14;
	ld.global.f32 	%f28, [%rd3+896];
	setp.gt.f32 	%p15, %f28, %f27;
	selp.f32 	%f29, %f28, %f27, %p15;
	selp.b32 	%r15, 14, %r14, %p15;
	ld.global.f32 	%f30, [%rd3+960];
	setp.gt.f32 	%p16, %f30, %f29;
	selp.b32 	%r16, 15, %r15, %p16;
	st.global.u32 	[%rd4], %r16;

$L__BB2_2:
	ret;

}
	// .globl	_Z22ann_scalar_layer1_reluPK13__nv_bfloat16S1_S1_PS_
.visible .entry _Z22ann_scalar_layer1_reluPK13__nv_bfloat16S1_S1_PS_(
	.param .u64 _Z22ann_scalar_layer1_reluPK13__nv_bfloat16S1_S1_PS__param_0,
	.param .u64 _Z22ann_scalar_layer1_reluPK13__nv_bfloat16S1_S1_PS__param_1,
	.param .u64 _Z22ann_scalar_layer1_reluPK13__nv_bfloat16S1_S1_PS__param_2,
	.param .u64 _Z22ann_scalar_layer1_reluPK13__nv_bfloat16S1_S1_PS__param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<69>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd1, [_Z22ann_scalar_layer1_reluPK13__nv_bfloat16S1_S1_PS__param_0];
	ld.param.u64 	%rd2, [_Z22ann_scalar_layer1_reluPK13__nv_bfloat16S1_S1_PS__param_1];
	ld.param.u64 	%rd3, [_Z22ann_scalar_layer1_reluPK13__nv_bfloat16S1_S1_PS__param_2];
	ld.param.u64 	%rd4, [_Z22ann_scalar_layer1_reluPK13__nv_bfloat16S1_S1_PS__param_3];
	cvta.to.global.u64 	%rd5, %rd1;
	cvta.to.global.u64 	%rd6, %rd2;
	cvta.to.global.u64 	%rd7, %rd3;
	mov.u32 	%r2, %tid.x;
	mul.wide.s32 	%rd8, %r2, 2;
	add.s64 	%rd9, %rd7, %rd8;
	shl.b32 	%r3, %r2, 4;
	ld.global.u16 	%rs4, [%rd9];
	mul.wide.s32 	%rd10, %r3, 2;
	add.s64 	%rd11, %rd6, %rd10;
	ld.global.u16 	%rs2, [%rd11];
	ld.global.u16 	%rs3, [%rd5];
	// begin inline asm
	{fma.rn.bf16 %rs1,%rs2,%rs3,%rs4;
}
	// end inline asm
	ld.global.u16 	%rs6, [%rd11+2];
	ld.global.u16 	%rs7, [%rd5+2];
	// begin inline asm
	{fma.rn.bf16 %rs5,%rs6,%rs7,%rs1;
}
	// end inline asm
	ld.global.u16 	%rs10, [%rd11+4];
	ld.global.u16 	%rs11, [%rd5+4];
	// begin inline asm
	{fma.rn.bf16 %rs9,%rs10,%rs11,%rs5;
}
	// end inline asm
	ld.global.u16 	%rs14, [%rd11+6];
	ld.global.u16 	%rs15, [%rd5+6];
	// begin inline asm
	{fma.rn.bf16 %rs13,%rs14,%rs15,%rs9;
}
	// end inline asm
	ld.global.u16 	%rs18, [%rd11+8];
	ld.global.u16 	%rs19, [%rd5+8];
	// begin inline asm
	{fma.rn.bf16 %rs17,%rs18,%rs19,%rs13;
}
	// end inline asm
	ld.global.u16 	%rs22, [%rd11+10];
	ld.global.u16 	%rs23, [%rd5+10];
	// begin inline asm
	{fma.rn.bf16 %rs21,%rs22,%rs23,%rs17;
}
	// end inline asm
	ld.global.u16 	%rs26, [%rd11+12];
	ld.global.u16 	%rs27, [%rd5+12];
	// begin inline asm
	{fma.rn.bf16 %rs25,%rs26,%rs27,%rs21;
}
	// end inline asm
	ld.global.u16 	%rs30, [%rd11+14];
	ld.global.u16 	%rs31, [%rd5+14];
	// begin inline asm
	{fma.rn.bf16 %rs29,%rs30,%rs31,%rs25;
}
	// end inline asm
	ld.global.u16 	%rs34, [%rd11+16];
	ld.global.u16 	%rs35, [%rd5+16];
	// begin inline asm
	{fma.rn.bf16 %rs33,%rs34,%rs35,%rs29;
}
	// end inline asm
	ld.global.u16 	%rs38, [%rd11+18];
	ld.global.u16 	%rs39, [%rd5+18];
	// begin inline asm
	{fma.rn.bf16 %rs37,%rs38,%rs39,%rs33;
}
	// end inline asm
	ld.global.u16 	%rs42, [%rd11+20];
	ld.global.u16 	%rs43, [%rd5+20];
	// begin inline asm
	{fma.rn.bf16 %rs41,%rs42,%rs43,%rs37;
}
	// end inline asm
	ld.global.u16 	%rs46, [%rd11+22];
	ld.global.u16 	%rs47, [%rd5+22];
	// begin inline asm
	{fma.rn.bf16 %rs45,%rs46,%rs47,%rs41;
}
	// end inline asm
	ld.global.u16 	%rs50, [%rd11+24];
	ld.global.u16 	%rs51, [%rd5+24];
	// begin inline asm
	{fma.rn.bf16 %rs49,%rs50,%rs51,%rs45;
}
	// end inline asm
	ld.global.u16 	%rs54, [%rd11+26];
	ld.global.u16 	%rs55, [%rd5+26];
	// begin inline asm
	{fma.rn.bf16 %rs53,%rs54,%rs55,%rs49;
}
	// end inline asm
	ld.global.u16 	%rs58, [%rd11+28];
	ld.global.u16 	%rs59, [%rd5+28];
	// begin inline asm
	{fma.rn.bf16 %rs57,%rs58,%rs59,%rs53;
}
	// end inline asm
	ld.global.u16 	%rs62, [%rd11+30];
	ld.global.u16 	%rs63, [%rd5+30];
	// begin inline asm
	{fma.rn.bf16 %rs61,%rs62,%rs63,%rs57;
}
	// end inline asm
	cvta.to.global.u64 	%rd12, %rd4;
	mov.f32 	%f1, 0f00000000;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs65, %f1;}

	// end inline asm
	// begin inline asm
	{.reg .b32 a,b;
  mov.b32 a, {0, %rs61};
  mov.b32 b, {0, %rs65};
  set.gt.f32.f32 %r1, a, b;}

	// end inline asm
	setp.eq.s32 	%p1, %r1, 0;
	add.s64 	%rd13, %rd12, %rd8;
	selp.b16 	%rs68, %rs65, %rs61, %p1;
	st.global.u16 	[%rd13], %rs68;
	ret;

}

